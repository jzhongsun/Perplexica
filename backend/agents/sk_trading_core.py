from enum import Enum
from pydantic import BaseModel, Field
from typing import Annotated, Dict, Any

from semantic_kernel.connectors.ai.chat_completion_client_base import (
    ChatCompletionClientBase,
)
from semantic_kernel.contents import ChatMessageContent, FunctionCallContent
from agents.sk_trading_prompts import reflection_system_prompt


class FinancialTradingAgentStep(Enum):
    ANALYSTS = "analysts"
    RESEARCHERS = "researchers"
    RISK_ANALYSIS = "risk_analysis"
    TRADER = "trader"
    END = "end"


# Researcher team state
class FinancialTradingInvestDebateState(BaseModel):
    bull_history: Annotated[str, "Bullish Conversation history"] = Field(default="")
    bear_history: Annotated[str, "Bearish Conversation history"] = Field(default="")
    history: Annotated[str, "Conversation history"] = Field(default="")
    current_response: Annotated[str, "Latest response"] = Field(default="")
    judge_decision: Annotated[str, "Final judge decision"] = Field(default="")
    count: Annotated[int, "Length of the current conversation"] = Field(default=0)


# Risk management team state
class FinancialTradingRiskDebateState(BaseModel):
    risky_history: Annotated[str, "Risky Agent's Conversation history"] = Field(
        default=""
    )
    safe_history: Annotated[str, "Safe Agent's Conversation history"] = Field(
        default=""
    )
    neutral_history: Annotated[str, "Neutral Agent's Conversation history"] = Field(
        default=""
    )
    history: Annotated[str, "Conversation history"] = Field(default="")
    latest_speaker: Annotated[str, "Analyst that spoke last"] = Field(default="")
    current_risky_response: Annotated[str, "Latest response by the risky analyst"] = (
        Field(default="")
    )
    current_safe_response: Annotated[str, "Latest response by the safe analyst"] = (
        Field(default="")
    )
    current_neutral_response: Annotated[
        str, "Latest response by the neutral analyst"
    ] = Field(default="")
    judge_decision: Annotated[str, "Judge's decision"] = Field(default="")
    count: Annotated[int, "Length of the current conversation"] = Field(default=0)


class FinancialTradingAgentState(BaseModel):
    next_step: FinancialTradingAgentStep = Field(
        default=FinancialTradingAgentStep.ANALYSTS,
        description="The next step in the researcher agent.",
    )
    messages: list[ChatMessageContent] = Field(
        default=[],
        description="The messages in the conversation.",
    )
    company_of_interest: Annotated[str, "Company that we are interested in trading"] = (
        Field(default="")
    )
    trade_date: Annotated[str, "What date we are trading at"] = Field(default="")

    sender: Annotated[str, "Agent that sent this message"] = Field(default="")

    # research step
    market_report: Annotated[str, "Report from the Market Analyst"] = Field(default="")
    sentiment_report: Annotated[str, "Report from the Social Media Analyst"] = Field(
        default=""
    )
    news_report: Annotated[
        str, "Report from the News Researcher of current world affairs"
    ] = Field(default="")
    fundamentals_report: Annotated[str, "Report from the Fundamentals Researcher"] = (
        Field(default="")
    )

    # researcher team discussion step
    investment_debate_state: Annotated[
        FinancialTradingInvestDebateState,
        "Current state of the debate on if to invest or not",
    ] = Field(default=FinancialTradingInvestDebateState())

    analyst_investment_plan: Annotated[str, "Plan generated by the Analyst"] = Field(
        default=""
    )
    trader_investment_plan: Annotated[str, "Plan generated by the Trader"] = Field(
        default=""
    )

    # risk management team discussion step
    risk_debate_state: Annotated[
        FinancialTradingRiskDebateState,
        "Current state of the debate on evaluating risk",
    ] = Field(default=FinancialTradingRiskDebateState())
    final_trade_decision: Annotated[str, "Final decision made by the Risk Analysts"] = (
        Field(default="")
    )


class FinancialTradingConditionalLogic:
    """Handles conditional logic for determining graph flow."""

    def __init__(self, max_debate_rounds=1, max_risk_discuss_rounds=1):
        """Initialize with configuration parameters."""
        self.max_debate_rounds = max_debate_rounds
        self.max_risk_discuss_rounds = max_risk_discuss_rounds

    def should_continue_market(
        self, state: FinancialTradingAgentState, last_message: ChatMessageContent
    ) -> bool:
        """Determine if market analysis should continue."""
        return any(isinstance(item, FunctionCallContent) for item in last_message.items)

    def should_continue_social(
        self, state: FinancialTradingAgentState, last_message: ChatMessageContent
    ) -> bool:
        """Determine if social media analysis should continue."""
        return any(isinstance(item, FunctionCallContent) for item in last_message.items)

    def should_continue_news(
        self, state: FinancialTradingAgentState, last_message: ChatMessageContent
    ) -> bool:
        """Determine if news analysis should continue."""
        return any(isinstance(item, FunctionCallContent) for item in last_message.items)

    def should_continue_fundamentals(
        self, state: FinancialTradingAgentState, last_message: ChatMessageContent
    ) -> bool:
        """Determine if fundamentals analysis should continue."""
        return any(isinstance(item, FunctionCallContent) for item in last_message.items)

    def should_continue_debate(
        self,
        state: FinancialTradingAgentState,
        debate_state: FinancialTradingInvestDebateState,
    ) -> bool:
        """Determine if debate should continue."""

        if (
            debate_state.count >= self.max_debate_rounds
        ):  # 3 rounds of back-and-forth between 2 agents
            return False
        return True

    def should_continue_risk_analysis(
        self,
        state: FinancialTradingAgentState,
        risk_debate_state: FinancialTradingRiskDebateState,
    ) -> bool:
        """Determine if risk analysis should continue."""
        if (
            risk_debate_state.count < 3 * self.max_risk_discuss_rounds
        ):  # 3 rounds of back-and-forth between 3 agents
            return True
        if risk_debate_state.latest_speaker.startswith("Risky"):
            return True
        if risk_debate_state.latest_speaker.startswith("Safe"):
            return True
        if risk_debate_state.latest_speaker.startswith("Neutral"):
            return False
        return False


class FinancialSituationMemory:
    def __init__(self, name, config):
        # if config["backend_url"] == "http://localhost:11434/v1":
        #     self.embedding = "nomic-embed-text"
        # else:
        #     self.embedding = "text-embedding-3-small"
        # self.client = OpenAI(base_url=config["backend_url"])
        # self.chroma_client = chromadb.Client(Settings(allow_reset=True))
        # self.situation_collection = self.chroma_client.create_collection(name=name)
        pass

    def get_embedding(self, text):
        """Get OpenAI embedding for a text"""

        # response = self.client.embeddings.create(
        #     model=self.embedding, input=text
        # )
        # return response.data[0].embedding
        return []

    def add_situations(self, situations_and_advice):
        """Add financial situations and their corresponding advice. Parameter is a list of tuples (situation, rec)"""

        # situations = []
        # advice = []
        # ids = []
        # embeddings = []

        # offset = self.situation_collection.count()

        # for i, (situation, recommendation) in enumerate(situations_and_advice):
        #     situations.append(situation)
        #     advice.append(recommendation)
        #     ids.append(str(offset + i))
        #     embeddings.append(self.get_embedding(situation))

        # self.situation_collection.add(
        #     documents=situations,
        #     metadatas=[{"recommendation": rec} for rec in advice],
        #     embeddings=embeddings,
        #     ids=ids,
        # )
        return []

    def get_memories(self, current_situation, n_matches=1):
        # """Find matching recommendations using OpenAI embeddings"""
        # query_embedding = self.get_embedding(current_situation)

        # results = self.situation_collection.query(
        #     query_embeddings=[query_embedding],
        #     n_results=n_matches,
        #     include=["metadatas", "documents", "distances"],
        # )

        # matched_results = []
        # for i in range(len(results["documents"][0])):
        #     matched_results.append(
        #         {
        #             "matched_situation": results["documents"][0][i],
        #             "recommendation": results["metadatas"][0][i]["recommendation"],
        #             "similarity_score": 1 - results["distances"][0][i],
        #         }
        #     )

        # return matched_results
        return []


class FinancialTradingReflector:
    """Handles reflection on decisions and updating memory."""

    def __init__(self, quick_thinking_llm: ChatCompletionClientBase):
        """Initialize the reflector with an LLM."""
        self.quick_thinking_llm = quick_thinking_llm
        self.reflection_system_prompt = self._get_reflection_prompt()

    def _get_reflection_prompt(self) -> str:
        """Get the system prompt for reflection."""
        return reflection_system_prompt

    def _extract_current_situation(self, current_state: Dict[str, Any]) -> str:
        """Extract the current market situation from the state."""
        curr_market_report = current_state["market_report"]
        curr_sentiment_report = current_state["sentiment_report"]
        curr_news_report = current_state["news_report"]
        curr_fundamentals_report = current_state["fundamentals_report"]

        return f"{curr_market_report}\n\n{curr_sentiment_report}\n\n{curr_news_report}\n\n{curr_fundamentals_report}"

    def _reflect_on_component(
        self, component_type: str, report: str, situation: str, returns_losses
    ) -> str:
        """Generate reflection for a component."""
        messages = [
            ("system", self.reflection_system_prompt),
            (
                "human",
                f"Returns: {returns_losses}\n\nAnalysis/Decision: {report}\n\nObjective Market Reports for Reference: {situation}",
            ),
        ]

        result = self.quick_thinking_llm.invoke(messages).content
        return result

    def reflect_bull_researcher(
        self, current_state, returns_losses, bull_memory: FinancialSituationMemory
    ):
        """Reflect on bull researcher's analysis and update memory."""
        situation = self._extract_current_situation(current_state)
        bull_debate_history = current_state["investment_debate_state"]["bull_history"]

        result = self._reflect_on_component(
            "BULL", bull_debate_history, situation, returns_losses
        )
        bull_memory.add_situations([(situation, result)])

    def reflect_bear_researcher(
        self, current_state, returns_losses, bear_memory: FinancialSituationMemory
    ):
        """Reflect on bear researcher's analysis and update memory."""
        situation = self._extract_current_situation(current_state)
        bear_debate_history = current_state["investment_debate_state"]["bear_history"]

        result = self._reflect_on_component(
            "BEAR", bear_debate_history, situation, returns_losses
        )
        bear_memory.add_situations([(situation, result)])

    def reflect_trader(
        self, current_state, returns_losses, trader_memory: FinancialSituationMemory
    ):
        """Reflect on trader's decision and update memory."""
        situation = self._extract_current_situation(current_state)
        trader_decision = current_state["trader_investment_plan"]

        result = self._reflect_on_component(
            "TRADER", trader_decision, situation, returns_losses
        )
        trader_memory.add_situations([(situation, result)])

    def reflect_invest_judge(
        self,
        current_state,
        returns_losses,
        invest_judge_memory: FinancialSituationMemory,
    ):
        """Reflect on investment judge's decision and update memory."""
        situation = self._extract_current_situation(current_state)
        judge_decision = current_state["investment_debate_state"]["judge_decision"]

        result = self._reflect_on_component(
            "INVEST JUDGE", judge_decision, situation, returns_losses
        )
        invest_judge_memory.add_situations([(situation, result)])

    def reflect_risk_manager(
        self,
        current_state,
        returns_losses,
        risk_manager_memory: FinancialSituationMemory,
    ):
        """Reflect on risk manager's decision and update memory."""
        situation = self._extract_current_situation(current_state)
        judge_decision = current_state["risk_debate_state"]["judge_decision"]

        result = self._reflect_on_component(
            "RISK JUDGE", judge_decision, situation, returns_losses
        )
        risk_manager_memory.add_situations([(situation, result)])


class FinancialTradingSignalProcessor:
    """Processes trading signals to extract actionable decisions."""

    def __init__(self, quick_thinking_llm: ChatCompletionClientBase):
        """Initialize with an LLM for processing."""
        self.quick_thinking_llm = quick_thinking_llm

    def process_signal(self, full_signal: str) -> str:
        """
        Process a full trading signal to extract the core decision.

        Args:
            full_signal: Complete trading signal text

        Returns:
            Extracted decision (BUY, SELL, or HOLD)
        """
        messages = [
            (
                "system",
                "You are an efficient assistant designed to analyze paragraphs or financial reports provided by a group of analysts. Your task is to extract the investment decision: SELL, BUY, or HOLD. Provide only the extracted decision (SELL, BUY, or HOLD) as your output, without adding any additional text or information.",
            ),
            ("human", full_signal),
        ]

        return self.quick_thinking_llm.invoke(messages).content
